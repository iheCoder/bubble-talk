# BubbleTalk 会话编排器技术实现方案（Session Orchestrator｜Go｜语音原生｜对话一等公民）

> 目标：把“用户语音输入 + 工具交互（答题/分叉/退出）+ 系统语音输出”统一为 **一条可回放、可验收的会话事件流**，并在后端用确定性的编排逻辑驱动导演状态机与工具。
>
> 本文只描述后端（Go）会话编排器如何实现。全局后端架构见：`docs/第一阶段技术实现方案.md`。

---

## 1. 会话编排器是什么（定位与边界）

### 1.1 定位：它不是“聊天 API”，而是“会话剪辑台”

会话编排器是系统的“剪辑台/现场助导”，它做三件事：

1. **把输入变成事实**：将语音 ASR final、答题、插话中断、退出请求等转换为带 `seq` 的事件并追加到 Timeline。
2. **把事实变成决策**：以 `reduce(state, event)` 回放/归约状态，构造 DirectorInput，调用 DirectorEngine 得到 DirectorPlan，并用 Guardrails 修正。
3. **把决策变成输出**：调用 ActorEngine 生成 **Prompt Instructions**，再通过 Realtime Gateway 驱动 gpt-realtime 生成语音并流式转发，最后把输出写回 Timeline。

一句话：**它负责“拍”，不负责“说”**。说话交给 Actor / gpt-realtime；决定交给 Director；但“系统到底做了什么”必须由编排器落库并可解释。

### 1.2 边界：哪些权力一定不能下放给模型

为了满足“对话一等公民”的可控与可验收，以下裁决必须在后端完成：

- 是否算作一次“有效用户输出动作”（影响 OutputClock 与学习闭环）
- `output_clock_sec >= 90` 时强制触发哪类输出（CHECK/FEYNMAN/TRANSFER/EXIT）
- 用户说“结束/我懂了”时必须触发 Exit Ticket（迁移检验）
- 工具调用（出题/评分/回收分叉栈）是否允许、何时允许、使用哪个模板
- 角色集合（RoleSet）边界：导演只能在泡泡固定角色集合内调度

---

## 2. 编排器要满足的核心不变量（Invariants）

这些不变量决定了系统是否真正“Conversation First”：

1. **Timeline 先行**：任何会影响学习推进的输入/输出都必须先追加到 Timeline，再触发推进逻辑（append-first）。
2. **单会话串行**：同一个 session 的推进必须串行执行（避免计划错位/重复出题/重复播报）。
3. **事件可幂等**：客户端重试、网络抖动、重连，都不能导致“同一次答题被记两次”“同一句助手语音播两次”。
4. **决策可解释**：每次 DirectorPlan 必须写入 Timeline（开发期强制，生产可采样/可开关）。
5. **输出可审计**：assistant 的文本字幕（以及关键音频段 ID/时间戳）必须进入 Timeline，才能做验收统计与回放。

---

## 3. 组件关系与数据流（高层）

```
Client (只连后端一个会话通道)
   |
   |  Session Stream (WS / WebRTC-to-backend)
   v
RealtimeGateway  <----->  OpenAI gpt-realtime (ASR/TTS)
   |                         ^
   | events                  | instructions / cancel
   v                         |
SessionOrchestrator ---------+
   |
   | append events
   v
TimelineStore (append-only) + SnapshotStore (latest SessionState)
```

编排器处在中心：它既接收来自 Gateway 的输入事件，也负责把“导演/演员的指令”送回 Gateway 驱动语音输出。

---

## 4. 核心数据模型（编排器视角）

> 关键点：**编排器只相信事件流**。快照只是缓存。

### 4.1 Event：统一承载“语音 + 工具 + 控制 + 输出”

建议把事件分成两类：

- **事实事件（Fact Events）**：进入学习推进与验收统计的“硬事实”
- **体验事件（UX Events）**：只影响体验，不作为学习事实（可落库但不参与核心裁决）

事实事件最小集合（第一阶段）：

- 输入：
  - `asr_final`（语音最终转写；唯一进入导演决策的语音输入）
  - `user_message`（文字回退）
  - `quiz_answer`（答题）
  - `barge_in`（插话中断）
  - `exit_requested`（退出）
- 输出：
  - `director_plan`（每轮决策的结构化计划）
  - `assistant_text`（字幕）
  - `assistant_audio_started/assistant_audio_ended`（语音输出边界，用于节奏/中断/回放对齐）
  - `quiz_delivered` / `exit_ticket_delivered`

体验事件（可选）：

- `asr_partial`（实时字幕）
- `playback_volume_changed`（客户端侧降音/静音）
- `client_focus_changed`（App 前后台）

建议 Event 统一包含：

- `seq`（后端分配的单调序号，幂等与回放的基石）
- `event_id`（客户端 UUID，可用于去重）
- `turn_id`（将一组事件归属到同一轮：user turn / assistant turn）
- `client_ts/server_ts`

### 4.2 SessionState：快照（cache），必须可由回放重建

`SessionState = Reduce(all_events)`，任何不能从事件流重建的字段都属于风险（会导致回放不一致）。

### 4.3 DirectorInput：导演读取的是“监控面板”，不是全文对话

编排器负责把最近事件归约成结构化摘要，避免把整个对话历史喂给导演造成不稳定。

最小摘要建议包含：

- 当前主目标/幕/拍点
- 最近一次 `asr_final` 文本
- OutputClock、Tension、Load、Fatigue
- 最近一次用户有效输出质量（规则评分）
- 分叉栈深度与待回收问题列表（简化）

---

## 5. 编排器主循环（最重要）

### 5.1 单会话串行模型（推荐：Session Worker / Actor Model）

实现建议：每个 session 一个 worker goroutine，所有事件按顺序进入它的 inbox：

- 优点：天然避免并发错位；代码简单；与“可回放状态机”相容
- 代价：需要做 session 生命周期管理（TTL、并发上限、清理）

抽象接口（示意）：

```go
type Orchestrator interface {
  // OnEvent 接收一个“已标准化”的事件（已分配 seq/ts），并可能产生 0..N 个副作用（如：发送给 Realtime、写回快照）。
  OnEvent(ctx context.Context, sessionID string, evt Event) error
}
```

### 5.2 append-first：事件落库顺序（强制）

收到任何输入事件时，固定顺序：

1. **Normalize**：补齐字段（server_ts、session_id、turn_id…）
2. **AssignSeq**：分配 `seq`（与 TimelineStore 原子化）
3. **Append Timeline**：追加事件
4. **Reduce**：对该事件做状态归约（或对全量回放做增量归约）
5. **Decide/Act**：若该事件触发推进，则进入导演/演员/工具流程

为什么必须 append-first：

- 崩溃恢复时，你只要回放 Timeline 就能恢复到一致状态；
- 任何一次“系统输出”都能被审计，不会出现“说了但没记”或“记了但没说”。

### 5.3 触发推进的事件（Trigger Events）

不是所有事件都需要触发“推进一轮”。第一阶段建议：

触发推进：

- `asr_final`
- `user_message`
- `quiz_answer`
- `exit_requested`

不触发推进（只更新体验/信号）：

- `asr_partial`
- `assistant_audio_started/ended`（通常只更新节奏与可打断状态）

### 5.4 一轮推进的确定性流水线（核心）

当收到 Trigger Event：

**Step 0：加载快照**

- 从 SnapshotStore 取最新 `SessionState`（或缓存于 worker 内存）

**Step 1：Reducer 更新 state**

- `state = reduce(state, evt)`
- 只做“事实归约”，不做任何外部调用

**Step 2：构造 DirectorInput**

- 从 `state + recent events` 生成结构化面板（见 4.3）

**Step 3：导演决策**

- `plan = DirectorEngine.Decide(input)`
- 立即把 `director_plan` 作为事件写入 Timeline（Fact Event）

**Step 4：Guardrails 修正（硬约束优先）**

典型硬约束（第一阶段必须）：

- OutputClock ≥ 90 秒：强制 `plan.output_action` 为输出类（Recap/Choice/Transfer/Feynman）
- 用户“结束/我懂了”（来自 asr_final 或 user_message）：强制 Exit Ticket
- Branch：默认入栈（QuestionStack push），并安排回收策略
- 角色集合：plan.next_role 必须属于该泡泡 RoleSet

**Step 5：演员生成脚本（Prompt Construction）**

- `actor_prompt = ActorEngine.ConstructPrompt(plan, role_profile, context)`
- 演员不再生成具体的台词文本，而是生成发给 GPT Realtime 的 Prompt。

**Step 6：驱动 Realtime 输出**

- 调用 `RealtimeGateway.SendInstructions(actor_prompt)`
- 监听 Realtime 的 `response.audio_transcript.done` 事件，将其作为 `assistant_text` 写入 Timeline。
- 监听 `response.audio.done` 事件，记录音频播放状态。

**Step 7：工具注入（可选）**

- 若 `plan.tool_plan` 包含 Quiz/Card，则通过 Gateway 下发工具指令（客户端渲染）。
- 工具展示事件写入 Timeline。

---

## 6. 关键技术难点与对策

### 6.1 实时流与事件流的对齐

GPT Realtime 是流式的，而我们的业务逻辑是事件驱动的。

**对策：**
- **ASR Final 作为唯一真理**：忽略中间的 partial transcript，只在收到 final 时触发业务逻辑。
- **Audio Transcript Done 作为助手输出**：将 GPT 生成的最终文本作为 `assistant_text` 记录，用于后续的回放和分析。
- **Barge-in 处理**：当检测到用户打断（`input_audio_buffer.speech_started`），立即发送 `response.cancel` 给 GPT，并记录 `barge_in` 事件。

### 6.2 延迟控制

多了一层 Director 和 Actor，延迟会不会太高？

**对策：**
- **并行处理**：Director 决策和 Actor Prompt 构建通常很快（纯逻辑或轻量级 LLM），主要延迟在 GPT Realtime 的生成。
- **流式输出**：GPT Realtime 本身支持流式输出，用户体验上是“边想边说”。
- **预加载**：对于确定的流程（如开场白、固定的转场），可以预先生成或缓存。

### 6.3 状态一致性

如何保证 GPT Realtime 的上下文与我们的 SessionState 一致？

**对策：**
- **Session Update**：在每一轮对话开始前，通过 `session.update` 更新 GPT 的 `instructions`，确保它知道当前的角色、目标和约束。
- **Context 清理**：定期清理 GPT 的上下文（`input_audio_buffer.clear`），避免 token 溢出和上下文混乱，依靠我们的 `SessionState` 来维护长期记忆和状态。

### 7.3 超时与兜底（第一阶段必须有）

外部依赖必须设置超时：

- Director（若接 Router LLM）
- Actor（若接文本模型）
- Realtime（ASR/TTS）
- Assessment（生成题目/评分）

超时兜底策略（示例）：

- Director 超时：使用 heuristic 默认 plan（CHECK + Recap）
- Actor 超时：使用模板兜底台词（短句 + 输出动作）
- Realtime 超时：返回纯文本（仍写 Timeline）

---

## 8. 工具交互如何进入同一条时间线（回答“能不能混在一个 API 中”）

### 8.1 结论

**必须混在一个会话通道里**，否则：

- 语音与答题无法对齐同一轮（turn_id/seq），回放与验收会崩；
- 插话中断会让工具状态错位（例如：题已出但用户插话打断，后端不知道该不该收题）。

### 8.2 工具事件的编排规则（第一阶段建议）

- `quiz_delivered`：由编排器写入（代表“系统确实出题了”）
- `quiz_answer`：由客户端上报，必须带 question_id/answer/client_ts
- `quiz_scored`：由编排器写入（规则评分也算 scored）

编排器以这些事件更新：

- misconception_tags（由选项映射）
- mastery_estimate（规则更新）
- output_quality（如要求“一句话解释”则按因果词/边界词打分）

---

## 9. Storage：TimelineStore + SnapshotStore（可回放落地）

### 9.1 两个存储的职责

- TimelineStore：append-only 事件流（事实源）
- SnapshotStore：最新 SessionState（缓存，加速加载）

### 9.2 接口建议（示意）

```go
type TimelineStore interface {
  Append(ctx context.Context, sessionID string, evt Event) (assignedSeq int64, err error)
  List(ctx context.Context, sessionID string, cursor string, limit int) (events []Event, nextCursor string, err error)
}

type SnapshotStore interface {
  Get(ctx context.Context, sessionID string) (*SessionState, error)
  Save(ctx context.Context, s *SessionState) error
}
```

### 9.3 回放策略

第一阶段可以先做“增量归约”：

- worker 内存持有 state
- 每 append 一个事件就 reduce 一次

当 worker 不存在或重启恢复时：

- 从 SnapshotStore 取快照 + cursor（可选）
- 从 TimelineStore 拉取 cursor 后事件并回放

---

## 10. 可观测性与验收统计（让“对话一等公民”可量化）

编排器应该产出以下关键指标（至少日志 + 可选 metrics）：

- `output_clock_sec` 分布（是否满足 90 秒约束）
- 每轮 `director_plan.next_beat/output_action` 分布（是否有结构化推进）
- `barge_in` 次数与发生位置（是否“压声播报”）
- `asr_final_latency_ms`（用户说完到拿到 final 的延迟）
- `assistant_first_audio_ms`（决定后到开始播报的延迟）
- 退出率、Exit Ticket 完成率、迁移题通过率

这些统计都应能从 Timeline 直接计算（这是验收的底座）。

---

## 11. 第一阶段实现清单（落地到工程任务）

按优先级：

1. 定义 Event schema（含 seq/turn_id/event_id），并固定“哪些事件是 Fact Events”
2. 实现 TimelineStore（先内存/文件）与 SnapshotStore（内存）
3. 实现 session worker：inbox 串行处理 + TTL 清理
4. 接入 Realtime Gateway（后端直连 gpt-realtime）：
   - asr_final → timeline → trigger
   - assistant audio/text → timeline → forward
   - barge-in cancel
5. Orchestrator 流水线（先 stub Director/Actor/Assessment）
6. Guardrails：90 秒输出 + “我懂了/结束”强制 Exit Ticket

