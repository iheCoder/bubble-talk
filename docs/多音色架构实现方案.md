# 多音色架构实现方案

## 概述

本方案实现了基于"一个角色一个独立 Realtime 连接"的多音色架构，解决了 OpenAI Realtime API 不支持在同一会话中切换音色的限制。

## 核心原理：文本镜像策略 (Text Mirroring Strategy)

### 问题
每个 Realtime 连接都有自己独立的对话上下文（conversation history）。如果不同步，角色切换时会出现"失忆"现象。

### 解决方案
**将所有真实发生过的"用户文本"和"助手文本"，以 text item 的形式同步给每一个 RoleConn**

这样做的好处：
- ✅ 所有角色连接随时保持一致的文本上下文
- ✅ 切换角色时不需要"补课重放"
- ✅ 即使某次只让 host 说话，economist/skeptic 也能"看到"发生了什么
- ✅ 实现简单，稳定可靠

注意事项：
- 同步的是**文本 item**，不是音频
- 不会触发 "assistant audio is present" 的 voice 更新问题
- 成本会上升：每次用户/助手文本都要写 N 份（N=角色数）

## 架构组件

### 1. RoleConn（角色连接）
`server/internal/gateway/role_conn.go`

代表一个特定角色的 Realtime 连接，每个 RoleConn：
- 在初始化时固定一个 voice（之后不再变化）
- 维护自己的 WebSocket 连接到 OpenAI
- 支持文本同步（SyncUserText / SyncAssistantText）
- 支持创建响应（CreateResponse）
- 支持取消响应（CancelResponse，用于插话中断）

关键方法：
```go
// 同步用户文本到该连接的对话历史
func (rc *RoleConn) SyncUserText(text string) error

// 同步助手文本到该连接的对话历史（排除自己说的）
func (rc *RoleConn) SyncAssistantText(text string, fromRole string) error

// 在该连接上创建响应（生成该角色的语音）
func (rc *RoleConn) CreateResponse(instructions string, metadata map[string]interface{}) error
```

### 2. VoicePool（音色池）
`server/internal/gateway/voice_pool.go`

管理多个角色连接，核心职责：
1. 为每个角色维护独立的 RoleConn
2. 维护一个 ASR 专用连接（只负责语音识别，不输出音频）
3. 实现"文本镜像"策略：让所有连接看起来共享同一段对话

关键方法：
```go
// 同步用户文本到所有角色连接
func (vp *VoicePool) SyncUserText(text string) error

// 同步助手文本到所有角色连接（除了说话的那个）
func (vp *VoicePool) SyncAssistantText(text string, fromRole string) error

// 在指定角色连接上创建响应
func (vp *VoicePool) CreateResponse(role string, instructions string, metadata map[string]interface{}) error

// 取消当前正在说话的角色的响应（用于插话中断）
func (vp *VoicePool) CancelCurrentResponse() error
```

### 3. MultiVoiceGateway（多音色网关）
`server/internal/gateway/multi_voice_gateway.go`

整合了 VoicePool，提供完整的网关功能：
- 维护客户端 ↔ 后端的 WebSocket 连接
- 维护后端 ↔ OpenAI 的多个 Realtime 连接
- 路由音频流和事件
- 处理插话中断

## 数据流

### 用户说话 → AI 回复的完整流程

```
1. 用户音频输入
   客户端 --[音频流]--> MultiVoiceGateway --[append]--> ASRConn

2. 语音识别
   ASRConn --[transcription.completed]--> MultiVoiceGateway
   
3. 文本镜像（关键！）
   MultiVoiceGateway --> VoicePool.SyncUserText()
   VoicePool --> RoleConn["host"].SyncUserText()
   VoicePool --> RoleConn["economist"].SyncUserText()
   VoicePool --> RoleConn["skeptic"].SyncUserText()
   
   每个 RoleConn 发送:
   {
     "type": "conversation.item.create",
     "item": {
       "type": "message",
       "role": "user",
       "content": [{"type": "input_text", "text": "用户说的话"}]
     }
   }

4. 导演决策
   MultiVoiceGateway --[asr_final event]--> Orchestrator
   Orchestrator --> Director.Decide() 
   决策结果：下一句由 "economist" 说

5. Actor 生成 Prompt
   Orchestrator --> Actor.BuildPrompt()
   生成针对 economist 的 instructions

6. 发送指令到指定角色
   Orchestrator --> MultiVoiceGateway.SendInstructions(instructions, {role: "economist"})
   MultiVoiceGateway --> VoicePool.CreateResponse("economist", instructions, metadata)
   VoicePool --> RoleConn["economist"].CreateResponse()
   
   RoleConn["economist"] 发送:
   {
     "type": "response.create",
     "response": {
       "modalities": ["text", "audio"],
       "instructions": "...",
       // 注意：这里不设置 voice，因为已经在 session.update 时固定了
     }
   }

7. AI 生成音频
   RoleConn["economist"] <--[response.audio.delta]-- OpenAI
   MultiVoiceGateway <--[音频流]-- RoleConn["economist"]
   客户端 <--[音频流]-- MultiVoiceGateway

8. 响应完成后再次文本镜像
   RoleConn["economist"] <--[response.done]-- OpenAI
   提取最终文本 --> VoicePool.SyncAssistantText(finalText, "economist")
   
   同步到其他角色:
   RoleConn["host"].SyncAssistantText(finalText, "economist")
   RoleConn["skeptic"].SyncAssistantText(finalText, "economist")
   
   每个 RoleConn 发送:
   {
     "type": "conversation.item.create",
     "item": {
       "type": "message",
       "role": "assistant",
       "content": [{"type": "text", "text": "经济学家说的话"}]
     }
   }
```

## 插话中断（Barge-in）处理

```
用户开始说话 --> 客户端发送 barge_in 事件
  --> MultiVoiceGateway.handleBargeIn()
  --> VoicePool.CancelCurrentResponse()
      - 找到当前正在说话的角色（如 "economist"）
      - 向 RoleConn["economist"] 发送 response.cancel
  --> 通知客户端清空音频缓冲区
```

## 配置示例

在 API Server 中使用 MultiVoiceGateway：

```go
// 在 server/internal/api/server.go 的 handleSessionStream 中

// 创建角色音色映射
roleProfiles := map[string]gateway.RoleProfile{
    "host": {
        Voice:  "alloy",  // 主持人用 alloy
        Avatar: "host.png",
    },
    "economist": {
        Voice:  "echo",   // 经济学家用 echo
        Avatar: "economist.png",
    },
    "skeptic": {
        Voice:  "shimmer", // 怀疑者用 shimmer
        Avatar: "skeptic.png",
    },
}

gwConfig := gateway.GatewayConfig{
    OpenAIAPIKey:                 config.OpenAI.APIKey,
    Model:                        "gpt-4o-realtime-preview-2024-12-17",
    RoleProfiles:                 roleProfiles,
    DefaultInstructions:          "...",
    InputAudioFormat:             "pcm16",
    OutputAudioFormat:            "pcm16",
    InputAudioTranscriptionModel: "whisper-1",
}

// 创建 MultiVoiceGateway
gw := gateway.NewMultiVoiceGateway(sessionID, clientConn, gwConfig)
```

## 资源消耗评估

以 3 个角色为例：
- **连接数**: 1 条 ASRConn + 3 条 RoleConn = **4 条 WebSocket / session**
- **同时在线 100 个 session**: 约 **400 条 OpenAI WebSocket**

需要关注：
- WebSocket 连接保活（ping/pong）
- 断线重连策略
- 后端实例的文件描述符/内存上限
- Token 消耗（每次文本会同步 N 份）

## 优化方向（未来可迭代）

### 策略 S2：最近 K 轮同步
当前实现是完整的文本镜像（所有历史都同步）。可以优化为：
- 只保留最近 K 轮对话（如 8-12 轮）
- 切换角色时按需补齐缺失的上下文
- 降低 Token 成本，但实现更复杂

### 摘要注入
对于长对话，可以：
- 对历史对话生成摘要
- 将摘要作为 system message 注入
- 减少每次同步的 token 数量

### 连接池复用
- 为不同 session 复用 RoleConn（相同角色）
- 需要处理会话隔离和状态清理

## 测试清单

- [ ] **多角色切换**: host → economist → skeptic 循环 5 轮，音色稳定且不同
- [ ] **快速切换**: 连续 3 次用户短句触发角色切换，无"无响应"
- [ ] **插话中断**: AI 说话时用户开口，AI 立即停止
- [ ] **文本一致性**: 检查所有 RoleConn 的对话历史是否一致
- [ ] **单连接故障**: 人为断开某个 RoleConn，系统应降级处理
- [ ] **并发压力**: 100 个并发 session，观察连接数和响应时间

## 与旧版 Gateway 的兼容性

当前保留了旧版 `gateway.Gateway`，新版 `MultiVoiceGateway` 实现了相同的接口：
- `SetEventHandler(handler EventHandler)`
- `Start(ctx context.Context) error`
- `SendInstructions(ctx context.Context, instructions string, metadata map[string]interface{}) error`
- `Close() error`
- `Done() <-chan struct{}`

可以通过配置选择使用哪个版本的 Gateway，实现平滑迁移。

## 总结

通过"一个角色一个连接 + 文本镜像"的架构：
- ✅ 彻底解决了 `cannot_update_voice` 问题
- ✅ 每个角色拥有稳定、可辨识的音色
- ✅ 所有角色共享一致的对话上下文
- ✅ 支持插话中断
- ✅ 实现简单，稳定可靠

代价是连接数和 Token 消耗的增加，但对于追求实时性和音色区分的应用场景，这是值得的。

