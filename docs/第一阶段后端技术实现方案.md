# BubbleTalk 第一阶段技术实现方案（后端 Go Only｜语音原生｜对话一等公民｜可直接开工）

> 本文只描述 **Go 后端**：它如何把“语音对话”变成系统的事实源，如何调度导演状态机与工具，并如何让整条会话可回放、可验收。
>
> 前端（Vue/RTC/录音播放/字幕 UI）请看：`docs/第一阶段前端实现方案（Vue）.md`。

---

## 0. 第一阶段目标与边界

### 0.1 第一阶段要交付什么（MVP-1）

第一阶段交付的是一个“能跑通、可验收”的 **语音对话学习引擎后端**：

1. **固定泡泡列表**（≤10 个，JSON 配置）
2. **会话编排器 Session Orchestrator（核心骨架）**
   - 事件驱动：接收语音/工具事件并归约状态
   - 统一裁决：何时推进主线、何时出题、何时退出
3. **导演状态机 DirectorEngine（核心决策）**
   - 心智状态识别（启发式为主，可选 LLM Router）
   - 插话分类（Clarify/Deepen/Branch/Meta/Off-topic）
   - Beat 选择（候选 + 打分 + 硬约束）
   - 输出动作强制（Output Ladder + OutputClock：90 秒必输出）
4. **演员生成 ActorEngine（表达层）**
   - 生成“台词脚本”（可朗读文本 + 用户动作提示 + fallback）
   - 语音输出由 gpt-realtime 完成，但“导演约束”必须由后端注入并可回放
5. **测评/工具极简（mock 也可）**
   - DIAGNOSE：固定 2 题（可口语化宣读 + 点击答题）
   - EXIT：固定 1 题迁移 + 1 句解释（强制）
6. **Realtime Gateway（语音原生必需）**
   - 客户端只连后端（单一会话通道）
   - 后端直连 gpt-realtime（ASR + TTS）并转发音频/事件
   - 插话中断（barge-in）作为一等事件进入事件流
7. **可回放时间线（Conversation Timeline）**
   - 以事件流为事实源：可审计、可解释、可验收

### 0.2 第一阶段明确不做（允许延后）

- 推荐系统、长期记忆、知识检索（可固定/Mock）
- 多角色同时发言/叠音（第一阶段每次只允许一个主讲角色）
- 复杂语音体验优化（VAD、超低延迟流式 TTS、离线包等）

---

## 1. 核心原则：什么叫“对话一等公民”（Conversation First）

在工程上它不是一句 slogan，而是三条硬约束：

1. **事实源 = 会话事件流（Event Log）**
   - 任何会影响学习推进的输入（语音最终转写、答题、插话、退出）必须进入同一条事件流。
   - `SessionState` 必须由 `reduce(state, event)` 归约得到（可回放、可回归）。
   - 这里的 Event/Timestamp/Seq 不是“为了记日志”，而是为了把系统从“随缘聊天”变成“可证明的状态机”：
     - **崩溃恢复**：只要 Timeline 在，就能回放重建到一致状态（不会出现“说了但没记/记了但没说”）。
     - **幂等与重试**：客户端/网关重试不会导致同一次输入被算两次（靠 `seq` 与 `event_id`）。
     - **验收与审计**：OutputClock/退出必迁移/出题次数等都可以从事件流计算并验收，而不是靠体验主观判断。
     - **调试与迭代**：当某轮对话“像跑偏了”，你能精确定位是哪一个事件触发了哪次决策与输出。
2. **语音优先，但裁决在后端**
   - gpt-realtime 负责 ASR/TTS 等“语音能力”，但 **是否推进、是否出题、是否退出、是否算有效输出** 由后端裁决。
3. **可验收**
   - “每 90 秒必触发一次用户输出动作”“用户说结束必出迁移检验”必须能从时间线中验证，而不是依赖主观体验。

---

## 2. 后端总体架构（组件与关系）

一句话：**Realtime Gateway 把语音变成事件；Orchestrator 用事件驱动 Director/Actor/Assessment；Timeline 支撑回放与验收。**

```
Client (only talks to Backend)
   |
   |  Session Stream (WS or WebRTC-to-backend)
   v
Realtime Gateway  <----->  OpenAI gpt-realtime (ASR/TTS + optional text)
   |
   |  Events (asr_final/quiz_answer/barge_in/...)
   v
Session Orchestrator
   |--> Reducer (state = reduce(state, event))
   |--> DirectorEngine (Decide -> DirectorPlan)
   |--> Guardrails (hard constraints, fix plan)
   |--> Assessment/Tools (quiz/exit ticket)
   |--> ActorEngine (script -> instructions)
   |--> Realtime Gateway (send instructions, stream audio back)
   v
Timeline Store (append-only events) + Session Store (latest snapshot)
```

模块职责在“交互过程中做什么”，请看第 5 节的“端到端时序”。

---

## 3. 数据模型（第一阶段最小但够用）

> 目标：少而强，且所有关键字段都能落到事件流里复盘。

### 3.1 Bubble（入口）

`configs/bubbles.json`

```json
[
  {
    "entry_id": "econ_weekend_overtime",
    "domain": "economics",
    "title": "周末加班到底值不值？",
    "hook": "你以为赚了800，可能亏了更贵的东西。",
    "primary_concept_id": "econ_opportunity_cost"
  }
]
```

### 3.2 SessionState（快照，不是事实源）

`SessionState` 是最新状态快照，用于快速加载；事实源是 Timeline（事件流）。

关键字段（示意）：

```go
type SessionState struct {
  SessionID string
  EntryID   string
  Domain    string

  MainObjective string
  Act           int
  Beat          string
  PacingMode    string

  MasteryEstimate float64
  MisconceptionTags []string

  OutputClockSec int
  LastOutputAt   time.Time

  TensionLevel  int
  CognitiveLoad int

  QuestionStack []BranchQuestion
  Turns         []Turn // 用于 UI 快速渲染，严肃回放以 Timeline 为准
}
```

### 3.3 Event（事实源：统一承载“语音 + 工具 + 控制”）

第一阶段的关键：**语音输入与答题等工具交互必须能混在同一个 API/通道里**，并通过 `seq/turn_id` 形成同一条时间线。

最小事件集合（示意）：

```go
type Event struct {
  Seq      int64
  Type     string // asr_final / quiz_answer / barge_in / exit_requested / ...
  TurnID   string
  ClientTS time.Time
  ServerTS time.Time

  Text       string // asr_final / user_message
  QuestionID string // quiz_answer
  Answer     string // quiz_answer
}
```

约束：

- `Seq` 必须单调递增（由后端分配），用于幂等与回放。
- `asr_partial` 不进入“导演决策事实源”，只作为体验事件（可选落库）。
- “推进决策”的唯一语音输入：`asr_final.text`。

> 常见误解澄清：
> - Timeline/Event 不是“额外的日志系统”，它就是你的“会话数据库”（append-only），SessionState 只是缓存快照。
> - 你越早把“事件先行 + 归约快照”跑通，后面接 Director/Actor/Assessment 才不会变成到处打补丁的 if-else。

### 3.4 DirectorPlan（导演输出：结构化、可校验、可回放）

```go
type DirectorPlan struct {
  UserMindState []string
  Intent        string

  NextBeat     string
  NextRole     string
  OutputAction string // Choice/Recap/Example/Boundary/Feynman/Transfer

  TalkBurstLimitSec int
  InterruptibleAfterMS int

  TensionGoal string
  LoadGoal    string

  StackAction string // keep/push/pop（第一阶段先字符串，后续再结构化）
  Notes       string
}
```

### 3.5 ActorReply（演员输出：脚本，不是自由发挥）

```json
{
  "speech_text": "……（短句、可朗读）",
  "interruptible_after_ms": 800,
  "user_action": { "type": "recap", "prompt": "用一句话复述，必须包含因为…所以…" },
  "quiz": null,
  "fallbacks": ["如果你卡住，就用这个句式：因为____，所以____。"]
}
```

---

## 4. API 设计（后端对外接口：HTTP + 会话通道 + timeline）

### 4.1 基础 HTTP

- `GET /api/bubbles`：固定泡泡列表
- `POST /api/sessions`：创建会话（返回 session_id + 初始快照 + diagnose 题）
- `POST /api/sessions/{id}/events`：文本回退/测试入口（非主路径）

### 4.2 会话通道（唯一主路径：Conversation Bus）

第一阶段建议先用 **WebSocket**（实现成本最低，足够支撑导演闭环）；如果你明确要 RTC，则做“客户端↔后端 WebRTC”。

**方案 A（推荐）：WebSocket**

- `GET /api/sessions/{id}/stream`（WebSocket）
  - 文本帧：JSON 事件（quiz_answer / barge_in / debug_toggle …）
  - 二进制帧：音频数据（Opus/PCM16，帧头携带 codec/seq/ts）

**方案 B（可选）：WebRTC（客户端只连后端）**

- `POST /api/sessions/{id}/rtc/offer`（HTTP：交换 SDP）
  - 音频轨：用户上行 + 系统下行
  - DataChannel：事件与工具交互

通道内必须支持“混合事件”：

- `audio_frame`（语音输入）
- `quiz_answer`（工具输入）
- `barge_in`（控制事件）
- `exit_requested`（退出）

**原则**：对外只暴露一个“会话通道”，让所有交互都在同一时间线里发生。

### 4.3 会话时间线（回放/审计/验收）

- `GET /api/sessions/{id}/timeline?cursor=...`

返回（示意）：

```json
{
  "cursor": "C_...",
  "events": [
    { "seq": 101, "type": "asr_final", "turn_id": "T_1", "text": "...", "ts": "..." },
    { "seq": 102, "type": "director_plan", "turn_id": "T_2", "plan": { "next_beat": "Check", "output_action": "Recap" }, "ts": "..." },
    { "seq": 103, "type": "assistant_text", "turn_id": "T_2", "text": "...", "ts": "..." },
    { "seq": 104, "type": "quiz_answer", "question_id": "diag_q1", "answer": "B", "ts": "..." }
  ]
}
```

---

## 5. 端到端交互：每个模块具体做什么（强结构）

这里用“从用户说话到系统回应”的完整生命周期，解释后端到底在忙什么。重点关注：**导演如何决策** 以及 **如何控制 Realtime 说出我们要它说的话**。

### 5.1 会话启动（初始化）

1. **Client -> Backend**: `POST /api/sessions { entry_id: "econ_101" }`
2. **Orchestrator**:
   - 创建 Session，加载初始配置（Act=1, Beat=开场）。
   - **关键动作**：连接 OpenAI Realtime，发送 `session.update` 指令，设置基础 Prompt（人设、禁止废话、声音风格）。
3. **Assessment**: 准备好 2 道诊断题（Diagnose Quiz）。
4. **Response**: 返回 session_id 和题目给前端。

### 5.2 语音输入（用户说话）

1. **Client -> Gateway**: 持续发送音频流（WebSocket Binary Frame）。
2. **Gateway**:
   - 将音频流透传给 OpenAI Realtime (`input_audio_buffer.append`)。
   - 此时 OpenAI 会自动进行 VAD（语音活动检测）。
3. **OpenAI -> Gateway**: 发送 `input_audio_buffer.speech_stopped`（用户说完了）和 `conversation.item.created`（ASR 转写结果）。
4. **Gateway**: 
   - 拿到最终文本（如：“我觉得是因为机会成本”）。
   - **拦截！** 不要让 OpenAI 自动回复（`turn_detection` 设为 `server_vad` 但不自动 commit，或者手动控制 `response.create`）。
   - 将文本封装为 `asr_final` 事件，推入 Timeline，**唤醒 Orchestrator**。

### 5.3 研讨回合（核心：后端如何接管大脑）

这是“对话一等公民”的核心。OpenAI 只是嘴和耳朵，**脑子在后端**。

触发条件：Orchestrator 收到 `asr_final`（用户说完）或 `quiz_answer`（用户做题）。

#### 第一步：导演决策 (DirectorEngine)
*不再是神秘的函数调用，而是具体的分类判断：*
1. **输入**：用户刚才说了什么（“我觉得是机会成本”）、当前在哪（Act 1 / 解释概念阶段）。
2. **判断**：
   - **意图识别**：用户是在回答问题？还是在提问？还是在闲聊？
   - **状态迁移**：如果用户答对了 -> 计划 = **"进入下一节 (NextBeat)"**。
   - **状态迁移**：如果用户答错了 -> 计划 = **"澄清误区 (Clarify)"**。
   - **硬约束**：如果用户已经连续听了 90秒 -> 计划 = **"强制提问 (Check)"**。
3. **输出 (DirectorPlan)**：`{ "action": "Clarify", "topic": "sunk_cost", "style": "Socratic" }`

#### 第二步：演员生成脚本 (ActorEngine)
*把导演的抽象指令变成 OpenAI 能听懂的 Prompt：*
1. **输入**：DirectorPlan (`Clarify`, `sunk_cost`)。
2. **生成 Prompt**：
   > "用户刚才混淆了沉没成本。请用苏格拉底式提问引导他。不要直接给答案。限制在 50 字以内。请用口语化的中文。"
3. **输出 (ActorScript)**：一段具体的 System Instruction 增量。

#### 第三步：指令注入 (Realtime Gateway)
*这是真正控制 Realtime 的地方：*
1. **Gateway -> OpenAI**: 发送 `response.create` 请求。
2. **Payload**:
   ```json
   {
     "response": {
       "instructions": "用户刚才混淆了沉没成本。请用苏格拉底式提问引导他..." 
       // 这里把 Actor 生成的 Prompt 动态注入，覆盖或追加到默认 Prompt
     }
   }
   ```
3. **OpenAI -> Gateway**: 返回生成的音频流 (`response.audio.delta`)。
4. **Gateway -> Client**: 转发音频流播放。

### 5.4 插话中断（Barge-in：用户抢话怎么办）

“Barge-in”就是**打断**。在全双工语音中，用户随时可能说话，系统必须立刻闭嘴。

1. **Client (VAD)**: 检测到用户开始说话（能量跃升）。
2. **Client -> Gateway**: 发送 `barge_in` 事件（或者直接发送新音频帧）。
3. **Gateway**:
   - **立刻动作**：向 OpenAI 发送 `response.cancel`（停止生成）。
   - **立刻动作**：向 Client 发送 `audio.clear`（清空未播放缓冲）。
   - **记录**：在 Timeline 记一笔 `barge_in`，告诉导演“用户很急”或“用户有异议”，下次决策时可能会缩短回复长度。

### 5.5 工具交互（做题）

1. **Client**: 用户点击屏幕选项 "B"。
2. **Gateway**: 收到 `quiz_answer: "B"`。
3. **Orchestrator**: 
   - 记录答案。
   - 再次触发 **5.3 研讨回合**。
   - 导演看到用户做题了，决定下一步是“夸奖并解释”还是“纠正错误”。
   - 生成新的 Prompt 注入 Realtime，让它说：“选得对！但这背后的逻辑其实是...”

### 5.6 退出

1. **用户说**：“我不想聊了” 或 “结束”。
2. **Director**: 识别意图 = `Exit`。
3. **Actor**: 生成 Prompt：“好的，但在走之前，我想确认你是否掌握了核心概念...”
4. **Assessment**: 推送一道 **Exit Ticket**（迁移测试题）到前端 UI。
5. **Gateway**: 注入 Prompt，播放挽留/引导语，同时前端弹出题目。

---

## 6. Director/Actor/Assessment 的实现边界（避免漂移）

第一阶段要坚持一条分工线：

- Director：只做“选择动作/拍点/工具/输出要求”，输出结构化计划（可校验）
- Actor：只做“按计划表达”，输出脚本（可朗读、短句、可打断）
- Assessment：只做“题目与评分”，并把结果写回 state
- gpt-realtime：只做“语音能力 +（可选）受控文本生成”，不直接拥有系统裁决权

导演细节与角色调度建议参考：

- `docs/导演状态机技术实现方案.md`
- `docs/导演如何调度“泡泡角色”.md`
- `docs/附件 A：导演状态机规则集（Director State Machine Rule Set）.md`
